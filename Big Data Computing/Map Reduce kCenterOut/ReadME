This is the final project for the Big Data Computing course, in which 
I implemented a 2-Round Map Reduce version of kCenterOut, to make it scalable and suitable for Big Data scenarios.

Specifically, the 2-Round Map Reduce version implements a coreset-based approach:
- Round 1: we extract a coreset from each partition by performing the Farthest First Traversal. We then merge all coresets togheter;
- Round 2: we execute kCenterOut on the set obtained from the previous round.

Interestingly, we observed that increasing the number of partitions improves both accuracy and computation time, as one would expect. 
We also note that for each execution, the first round weighs the most on the computation time, because of the FFT.
In this sense, increasing the number of partitions means running the FFT on smaller subsets, thus achieving higher accuracies and faster performances. 
In particular, the coreset approach allowed the algorithm to be run on a dataset with more than 1 million points in reasonable times.

Have a look yourself by testing the code on the datasets provided in this folder!
